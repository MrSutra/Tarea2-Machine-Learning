{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de Opiniones sobre Películas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargando librerias..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=UTF-8\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, time\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import WordNetLemmatizer, word_tokenize, data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "#comentar a siguiente linea si se tienen instalado en el path por defecto\n",
    "data.path.append('nltk_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(a) Dataframe**\n",
    "Se cargan los datos a analizar para crear el dataframe que se utilizará en el desarrollo del problema de análisis de opiniones de películas. Se utilizará un set de entrenamiento y un set de prueba.  Ambos documentos presentan dos columnas (sin contar el índice). La primera contiene el **Sentimiento** que toma valor de **+1** cuando es positivo y **-1** cuando es negativo. La segunda columna corresponde al texto u opinión escrita por una persona respecto a una película. El siguiente es un extracto de los datos\n",
    "\n",
    "|Sentiment|Text|\n",
    "|:------|------:-----|\n",
    "|-1|everything's serious , poetic , earnest and --...|\n",
    "|-1|narratively , trouble every day is a plodding ...|\n",
    "|+1|a truly wonderful tale combined with stunning ...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cargar datos\n",
    "train_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.train\"\n",
    "test_data_url = \"http://www.inf.utfsm.cl/~jnancu/stanford-subset/polarity.dev\"\n",
    "train_data_f = urllib.urlretrieve(train_data_url, \"train_data.csv\")\n",
    "test_data_f = urllib.urlretrieve(test_data_url, \"test_data.csv\")\n",
    "ftr = open(\"train_data.csv\", \"r\")\n",
    "fts = open(\"test_data.csv\", \"r\")\n",
    "\n",
    "# separar lineas en dos columnas: sentimiento y texto u opinion\n",
    "rows = [line.split(\" \",1) for line in ftr.readlines()]\n",
    "train_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "train_df['Sentiment'] = pd.to_numeric(train_df['Sentiment'])\n",
    "rows = [line.split(\" \",1) for line in fts.readlines()]\n",
    "test_df = pd.DataFrame(rows, columns=['Sentiment','Text'])\n",
    "test_df['Sentiment'] = pd.to_numeric(test_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez armados los dataframes, la cantidad de entradas que tiene cada uno es de 3554 filas y 2 columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros train set  (3554, 2)\n",
      "Cantidad de registros test set  (3554, 2)\n"
     ]
    }
   ],
   "source": [
    "print \"Cantidad de registros train set \",train_df.shape\n",
    "print \"Cantidad de registros test set \",test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(b) Stemming:**\n",
    "La siguiente función *word_extractor* devuelve una lista de las palabras contenidas en un determinad trozo de texto. Incluye las operaciones de lower-casing y stemming. Stemming es el proceso de reducir una palabra a su raiz (stem). En este caso se utilizará el algoritmo de Porter. Corta las palabras esperando encontrar un sufijo, incluyendo eliminación de afijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming\n",
      " love eat cake deni\n",
      " love eat cake\n",
      " love eat cake\n",
      " love eat cake\n",
      " n't love eat cake\n",
      " die last week . wa sensat\n",
      " cat , stem cake , faction run\n",
      "\n",
      "Sin Stemming\n",
      " love eat cake denied\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " died last week . sensational\n",
      " owned cats , stemming cakes , factionally running\n"
     ]
    }
   ],
   "source": [
    "def word_extractor(text,sw=True,stmmng=True):\n",
    "    porterstemmer = PorterStemmer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    if stmmng:\n",
    "        wordtokens = [ porterstemmer.stem(word.lower()) for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    else:\n",
    "        wordtokens = [ word.lower() for word in word_tokenize(text.decode('utf-8', 'ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if sw:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "        else:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print \"Stemming\"\n",
    "print word_extractor(\"I love to eat cake denied\")\n",
    "print word_extractor(\"I love eating cake\")\n",
    "print word_extractor(\"I loved eating the cake\")\n",
    "print word_extractor(\"I do not love eating cake\")\n",
    "print word_extractor(\"I don't love eating cake\")\n",
    "print word_extractor(\"You died last WeEk. It was sensational\")\n",
    "print word_extractor(\"owned cats, stemming cakes, factionally running\") \n",
    "\n",
    "print \"\\nSin Stemming\"\n",
    "print word_extractor(\"I love to eat cake denied\",stmmng=False)\n",
    "print word_extractor(\"I love eating cake\",stmmng=False)\n",
    "print word_extractor(\"I loved eating the cake\",stmmng=False)\n",
    "print word_extractor(\"I do not love eating cake\",stmmng=False)\n",
    "print word_extractor(\"I don't love eating cake\",stmmng=False)\n",
    "print word_extractor(\"You died last WeEk. It was sensational\",stmmng=False)\n",
    "print word_extractor(\"owned cats, stemming cakes, factionally running\",stmmng=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver como al aplicar stemming se cortan las palabras elimindo afijos y sufijos, además de los stop words. Sin stemming se eliminan las stop words pero las palabras quedan completas, no se cortan. En este caso la cantidad de palabras en el vocabulario aumenta pues, para una misma raiz de palabra, habrán muchas otras, en especial con las conjugaciones de verbos. Por ejemplo, running aplicando stemming se corta a run pero cuando no se aplica stemming queda solamente running, por lo tanto al tener running run quedarán como palabras separadas del vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(c) Lematizador: **\n",
    "Lematización es el proceso de convertir una palabra a su lema. Se eliminan formas en plural, femenino, conjugaciones. La palabra que se obtiene es una palabra del vocabulario presente en un diccionario. Por ejemplo el lema de nadé sería nadar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lematizador\n",
      " love eat cake denied\n",
      " love eating cake\n",
      " loved eating cake\n",
      " love eating cake\n",
      " n't love eating cake\n",
      " died last week . wa sensational\n",
      " owned cat , stemming cake , tiger run running\n"
     ]
    }
   ],
   "source": [
    "def word_extractor2(text,sw):\n",
    "    wordlemmatizer = WordNetLemmatizer()\n",
    "    commonwords = stopwords.words('english')\n",
    "    text = re.sub(r'([a-z])\\1+', r'\\1\\1',text)#substitute multiple letter by two\n",
    "    words = \"\"\n",
    "    wordtokens = [ wordlemmatizer.lemmatize(word.lower()) for word in word_tokenize(text.decode('utf-8','ignore')) ]\n",
    "    for word in wordtokens:\n",
    "        if sw:\n",
    "            if word not in commonwords:\n",
    "                words+=\" \"+word\n",
    "        else:\n",
    "            words+=\" \"+word\n",
    "    return words\n",
    "\n",
    "print \"\\nLematizador\"\n",
    "print word_extractor2(\"I love to eat cake denied\",True)\n",
    "print word_extractor2(\"I love eating cake\",True)\n",
    "print word_extractor2(\"I loved eating the cake\",True)\n",
    "print word_extractor2(\"I do not love eating cake\",True)\n",
    "print word_extractor2(\"I don't love eating cake\",True)\n",
    "print word_extractor2(\"You died last WeEk. It was sensational\",True)\n",
    "print word_extractor2(\"owned cats, stemming cakes, tigers run running\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunas palabras quedan similares a las procesadas por stemming pero a diferencia ahora son cortadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(d) Representación vectorial: **\n",
    "En el siguiente paso se construyó una función para generar una representación vectorial de los textos, aplicando lematización o stemming, con el eliminación y no limpieza de stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector_rep(method,sw=True):\n",
    "    if method == \"stemming\":\n",
    "        if sw:\n",
    "            texts_train    = [word_extractor(text) for text in train_df.Text]\n",
    "            texts_test     = [word_extractor(text) for text in test_df.Text] \n",
    "        else:\n",
    "            texts_train    = [word_extractor(text,sw=False) for text in train_df.Text]\n",
    "            texts_test     = [word_extractor(text,sw=False) for text in test_df.Text]  \n",
    "    elif method == \"lemmatisation\":\n",
    "        if sw:\n",
    "            texts_train    = [word_extractor2(text,True) for text in train_df.Text]\n",
    "            texts_test     = [word_extractor2(text,True) for text in test_df.Text]\n",
    "        else:\n",
    "            texts_train    = [word_extractor2(text,False) for text in train_df.Text]\n",
    "            texts_test     = [word_extractor2(text,False) for text in test_df.Text]\n",
    "\n",
    "    vectorizer     = CountVectorizer(ngram_range=(1, 1), binary='False')\n",
    "    vectorizer.fit(np.asarray(texts_train))\n",
    "    features_train = vectorizer.transform(texts_train)\n",
    "    features_test  = vectorizer.transform(texts_test)\n",
    "    labels_train   = np.asarray((train_df.Sentiment.astype(float)+1)/2.0)\n",
    "    labels_test    = np.asarray((test_df.Sentiment.astype(float)+1)/2.0)\n",
    "    vocab          = vectorizer.get_feature_names()\n",
    "    dist_train     = list(np.array(features_train.sum(axis=0)).reshape(-1,))\n",
    "    return features_train, features_test, labels_train, labels_test, vocab, dist_train "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenida la representación vectorial del training set y test set se puede ver cuales son las diez palabras más frecuentes dentro de las opiniones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set de entrenamiento:  (3554, 9663)\n",
      "Set de pruebas:  (3554, 9663)\n",
      "\n",
      "----\n",
      "Palabras más frecuentes en set de entrenamiento\n",
      "Repeticiones\tPalabra\n",
      "566\t\tfilm\n",
      "481\t\tmovie\n",
      "246\t\tone\n",
      "245\t\tlike\n",
      "224\t\tha\n",
      "183\t\tmake\n",
      "176\t\tstory\n",
      "163\t\tcharacter\n",
      "145\t\tcomedy\n",
      "143\t\ttime\n",
      "\n",
      "Palabras más frecuentes en set de prueba\n",
      "Repeticiones\tPalabra\n",
      "558\t\tfilm\n",
      "540\t\tmovie\n",
      "250\t\tone\n",
      "238\t\tha\n",
      "230\t\tlike\n",
      "197\t\tstory\n",
      "175\t\tcharacter\n",
      "165\t\ttime\n",
      "161\t\tmake\n",
      "134\t\tcomedy\n"
     ]
    }
   ],
   "source": [
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep(\"lemmatisation\")\n",
    "print \"Set de entrenamiento: \",features_train.shape\n",
    "print \"Set de pruebas: \", features_test.shape\n",
    "train_tags = []\n",
    "test_tags = []\n",
    "dist_test = list(np.array(features_test.sum(axis=0)).reshape(-1,))\n",
    "\n",
    "for tag, count in zip(vocab, dist_train):\n",
    "    train_tags.append([count, tag])\n",
    "for tag, count in zip(vocab, dist_test):\n",
    "    test_tags.append([count, tag])\n",
    "\n",
    "train_tags.sort(reverse=True)\n",
    "test_tags.sort(reverse=True)\n",
    "print \"\\n----\\nPalabras más frecuentes en set de entrenamiento\\nRepeticiones\\tPalabra\"\n",
    "for i in range(10):\n",
    "    print \"%d\\t\\t%s\" % (train_tags[i][0], train_tags[i][1])\n",
    "print \"\\nPalabras más frecuentes en set de prueba\\nRepeticiones\\tPalabra\"\n",
    "for i in range(10):\n",
    "    print \"%d\\t\\t%s\" % (test_tags[i][0], test_tags[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ambos casos, train set y test set, están compuestos de 9663 palabras. En ambos casos se repiten las palabras, estando film y movie obviamente arriba del ranking, pues se está hablando de películas. El resto de las palabras tienen diferente orden y esto depende de como fueron analizadas por stemming y lematizador. Los métodos no lograban llegar, en todos los casos, a la misma raiz de las palabras. Así, la distribución de ellas cambia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### **(e) Función de desempeño**:\n",
    "\n",
    "Se procedió a crear la función de evaluación de desempeño para los clasificadores. Las métricas que utiliza la función *classification_report* de la librería sklearn son tres:\n",
    "\n",
    "1. **Precision**: es el ratio de verdaderos positivos divididos por la cantidad de casos positivos (falsos positivos +verdaderos positivos). Es la habilidad del clasificador de no etiquetar una muestra como positiva cuando esta es negativa.\n",
    "2. **Recall**: Es el ratio entre verdaderos positivos y la cantidad de verdaderos esperados. Es la habilidad del clasificador de encontrar todos las muestras positivas. \n",
    "3. **f1-score**: Es la medida de precisión que tiene un test. Tiene su mejor caso cuando alcanza uno y su peor caso cuando alcanza cero. Está relacionada con precision y recall, calculándose de la siguiente manera: \n",
    "\\begin{equation*}\n",
    "F1 = 2 * \\frac{precision * recall}{precision + recall}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_the_model(model,x,y,xt,yt,text):\n",
    "    acc_tr = model.score(x,y)\n",
    "    acc_test = model.score(xt[:-1],yt[:-1])\n",
    "    print \"Training Accuracy %s: %f\"%(text,acc_tr)\n",
    "    print \"Test Accuracy %s: %f\"%(text,acc_test)\n",
    "    print \"Detailed Analysis Testing Results ...\"\n",
    "    print (classification_report(yt, model.predict(xt), target_names=['+','-']))\n",
    "    #computar precisión recall fscore y soporte para cada clase\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(yt, model.predict(xt))\n",
    "\n",
    "    if text == 'BernoulliNB':\n",
    "        BNB = []\n",
    "        BNB.append((acc_tr,acc_test,np.mean(precision),np.mean(recall),np.mean(fscore)))\n",
    "    elif text == 'MULTINOMIAL':\n",
    "        multinomial = []\n",
    "        multinomial.append((acc_tr,acc_test,np.mean(precision),np.mean(recall),np.mean(fscore)))\n",
    "    elif text == 'LOGISTIC':\n",
    "        logistic = []\n",
    "        logistic.append((acc_tr,acc_test,np.mean(precision),np.mean(recall),np.mean(fscore)))\n",
    "    elif text == 'SVM':\n",
    "        svm = []\n",
    "        svm.append((acc_tr,acc_test,np.mean(precision),np.mean(recall),np.mean(fscore)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(f) Clasificador Bayesiano Ingenuo (binario): **\n",
    "Se corrió el clasificador Naive Bayes para stemming y lematizador, dejando y eliminando stop words para ver su comportamiento. Las features son transformadas a 1 y 0 para su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes - BernoulliNB\n",
      "Lematizador con Stopwords\n",
      "Training Accuracy BernoulliNB: 0.958638\n",
      "Test Accuracy BernoulliNB: 0.738531\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Lematizador sin Stopwords\n",
      "Training Accuracy BernoulliNB: 0.955262\n",
      "Test Accuracy BernoulliNB: 0.748663\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.76      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Stemming con Stopwords\n",
      "Training Accuracy BernoulliNB: 0.942881\n",
      "Test Accuracy BernoulliNB: 0.747819\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.74      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Stemming con Stopwords\n",
      "Training Accuracy BernoulliNB: 0.938098\n",
      "Test Accuracy BernoulliNB: 0.762173\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.77      0.76      0.77      1803\n",
      "          -       0.76      0.76      0.76      1751\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3554\n",
      "\n",
      "Conjunto aleatorio de prueba\n",
      "[ 0.04078004  0.95921996] about a boy vividly recalls the cary grant of room for one more , houseboat and father goose in its affectionate depiction of the gentle war between a reluctant , irresponsible man and the kid who latches onto him .\n",
      "\n",
      "[ 0.06638807  0.93361193] director andrew niccol . . . demonstrates a wry understanding of the quirks of fame . his healthy sense of satire is light and fun . . . .\n",
      "\n",
      "[ 0.57636999  0.42363001] a loquacious and dreary piece of business .\n",
      "\n",
      "[ 0.17726129  0.82273871] it's fascinating to see how bettany and mcdowell play off each other .\n",
      "\n",
      "[ 0.00206945  0.99793055] anchored by friel and williams's exceptional performances , the film's power lies in its complexity . nothing is black and white .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def do_NAIVE_BAYES(x,y,xt,yt):\n",
    "    model = BernoulliNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"BernoulliNB\")\n",
    "    return model\n",
    "\n",
    "BNB = []\n",
    "print \"Naive Bayes - BernoulliNB\"\n",
    "print \"Lematizador con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation')\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "\n",
    "print \"Lematizador sin Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation',sw=False)\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming')\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming',sw=False)\n",
    "model=do_NAIVE_BAYES(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Conjunto aleatorio de prueba\"\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede ver como BernoulliNB tiene un alto valor de accuracy en el training set pero para el test set este disminuye en una considerable proporción. Esto quiere decir que el modelo está sobreajustado respecto a los datos. Como está sobreajustado la calidad de predicción no es muy buena, información que se puede corroborrar con los valores de precision y recall.\n",
    "\n",
    "Stemming tiene resultados similares a los de lematización, siendo este último el mejor cuando no se eliminan los stop words. Para seleccionar el mejor, dado que los resultados son cercanos, se puede ocupar un criterio de tiempo de ejecución.\n",
    "\n",
    "Para todos los casos la accuracy de entrenamiento es mejor que la de testing. Una conclusión, sin ver los datos, es que pueden existir palabras que no estén presentes en el test de entrenamiento. En este caso habría problema clasificándolas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(g) Clasificador bayesiano ingenuo Multinomial:**\n",
    "Se corrió el clasificador Multinomial Naive Bayes para stemming y lematizador, dejando y eliminando stop words para ver su comportamiento. Las features son transformadas a 1 y 0 para su funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BI Multinomial\n",
      "Lematizador con Stopwords\n",
      "Training Accuracy MULTINOMIAL: 0.959482\n",
      "Test Accuracy MULTINOMIAL: 0.740782\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.73      0.74      1803\n",
      "          -       0.73      0.75      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Lematizador sin Stopwords\n",
      "Training Accuracy MULTINOMIAL: 0.955543\n",
      "Test Accuracy MULTINOMIAL: 0.747537\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.75      0.75      1803\n",
      "          -       0.74      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Stemming con Stopwords\n",
      "Training Accuracy MULTINOMIAL: 0.942319\n",
      "Test Accuracy MULTINOMIAL: 0.749789\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.75      0.75      1803\n",
      "          -       0.74      0.75      0.75      1751\n",
      "\n",
      "avg / total       0.75      0.75      0.75      3554\n",
      "\n",
      "Stemming sin Stopwords\n",
      "Training Accuracy MULTINOMIAL: 0.940630\n",
      "Test Accuracy MULTINOMIAL: 0.759921\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.76      0.77      0.76      1803\n",
      "          -       0.76      0.75      0.76      1751\n",
      "\n",
      "avg / total       0.76      0.76      0.76      3554\n",
      "\n",
      "Conjunto aleatorio de prueba\n",
      "[ 0.0267729  0.9732271] the movie understands like few others how the depth and breadth of emotional intimacy give the physical act all of its meaning and most of its pleasure .\n",
      "\n",
      "[ 0.0962351  0.9037649] eight legged freaks won't join the pantheon of great monster/science fiction flicks that we have come to love . . .\n",
      "\n",
      "[ 0.65842807  0.34157193] great fun both for sports aficionados and for ordinary louts whose idea of exercise is climbing the steps of a stadium-seat megaplex .\n",
      "\n",
      "[ 0.65000847  0.34999153] some body smacks of exhibitionism more than it does cathartic truth telling .\n",
      "\n",
      "[ 0.03701206  0.96298794] this isn't exactly profound cinema , but it's good-natured and sometimes quite funny .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def do_MULTINOMIAL(x,y,xt,yt):\n",
    "    model = MultinomialNB()\n",
    "    model = model.fit(x, y)\n",
    "    score_the_model(model,x,y,xt,yt,\"MULTINOMIAL\")\n",
    "    return model\n",
    "\n",
    "multinomial = []\n",
    "print \"BI Multinomial\"\n",
    "print \"Lematizador con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation')\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "\n",
    "print \"Lematizador sin Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation',sw=False)\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming')\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming sin Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming',sw=False)\n",
    "model=do_MULTINOMIAL(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Conjunto aleatorio de prueba\"\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay mucha diferencia entre multinomial y el método anteriormente usado. Se repite que el mejor comportamiento lo lematización sin stop words. La accurazy en este caso es levemente mejor. Aún así, las métricas de precision, recall, f1-score no cambian mucho.\n",
    "\n",
    "Entre bayes solo y multinomial la diferencia es que, para Naive Bayes se necesita estimar la distribución de datos y clases. Para el caso de Multinomial simplemente se asume una distribución multinomial para todos los pares de datos. En casos de conteo de palabras es mejor el multinomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(h) Regresión logistica regularizada **\n",
    "\n",
    "El siguiente método a utilizar es Regresión logística regularizada. El penalty escogido es la noma $l_2$. Este penalizador se acompaña de un parámetro C que indica el nivel de penalización que se aplicará. Para valores menores de C la penalización es mayor. Con esto se facilita la obtención de características representativas del modelo.\n",
    "\n",
    "Para valores de C grandes es clara la presencia de overfitting. Cuando toma valores más pequeños se regulariza más por lo que se obtiene un mejor modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regresión Logística regularizada\n",
      "Lematizador con Stopwords\n",
      "Usando C= 0.010000\n",
      "Training Accuracy LOGISTIC: 0.784468\n",
      "Test Accuracy LOGISTIC: 0.678863\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.67      0.73      0.70      1803\n",
      "          -       0.69      0.63      0.66      1751\n",
      "\n",
      "avg / total       0.68      0.68      0.68      3554\n",
      "\n",
      "Usando C= 0.100000\n",
      "Training Accuracy LOGISTIC: 0.892234\n",
      "Test Accuracy LOGISTIC: 0.719111\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.72      0.71      0.71      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.989589\n",
      "Test Accuracy LOGISTIC: 0.721362\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.72      0.72      1803\n",
      "          -       0.72      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Lematizador sin Stopwords\n",
      "Usando C= 0.010000\n",
      "Training Accuracy LOGISTIC: 0.734102\n",
      "Test Accuracy LOGISTIC: 0.671827\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.68      0.68      0.68      1803\n",
      "          -       0.67      0.66      0.67      1751\n",
      "\n",
      "avg / total       0.67      0.67      0.67      3554\n",
      "\n",
      "Usando C= 0.100000\n",
      "Training Accuracy LOGISTIC: 0.879572\n",
      "Test Accuracy LOGISTIC: 0.718548\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.72      0.72      0.72      1803\n",
      "          -       0.71      0.72      0.72      1751\n",
      "\n",
      "avg / total       0.72      0.72      0.72      3554\n",
      "\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.987901\n",
      "Test Accuracy LOGISTIC: 0.736279\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.73      0.74      1803\n",
      "          -       0.73      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Stemming con StopWords\n",
      "Usando C= 0.010000\n",
      "Training Accuracy LOGISTIC: 0.782217\n",
      "Test Accuracy LOGISTIC: 0.690684\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.68      0.73      0.70      1803\n",
      "          -       0.70      0.65      0.68      1751\n",
      "\n",
      "avg / total       0.69      0.69      0.69      3554\n",
      "\n",
      "Usando C= 0.100000\n",
      "Training Accuracy LOGISTIC: 0.880135\n",
      "Test Accuracy LOGISTIC: 0.731213\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.75      0.74      1803\n",
      "          -       0.73      0.71      0.72      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.981148\n",
      "Test Accuracy LOGISTIC: 0.735153\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.74      0.74      0.74      1803\n",
      "          -       0.73      0.73      0.73      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Stemming sin StopWords\n",
      "Usando C= 0.010000\n",
      "Training Accuracy LOGISTIC: 0.741418\n",
      "Test Accuracy LOGISTIC: 0.678019\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.68      0.69      0.68      1803\n",
      "          -       0.68      0.67      0.67      1751\n",
      "\n",
      "avg / total       0.68      0.68      0.68      3554\n",
      "\n",
      "Usando C= 0.100000\n",
      "Training Accuracy LOGISTIC: 0.874226\n",
      "Test Accuracy LOGISTIC: 0.732620\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.73      0.74      0.74      1803\n",
      "          -       0.73      0.72      0.73      1751\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3554\n",
      "\n",
      "Usando C= 1.000000\n",
      "Training Accuracy LOGISTIC: 0.981429\n",
      "Test Accuracy LOGISTIC: 0.743597\n",
      "Detailed Analysis Testing Results ...\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          +       0.75      0.74      0.75      1803\n",
      "          -       0.74      0.74      0.74      1751\n",
      "\n",
      "avg / total       0.74      0.74      0.74      3554\n",
      "\n",
      "Conjunto aleatorio de prueba\n",
      "[ 0.67616121  0.32383879] despite the pyrotechnics , narc is strictly by the book .\n",
      "\n",
      "[ 0.43546562  0.56453438] williams plays sy , another of his open-faced , smiling madmen , like the killer in insomnia . he does this so well you don't have the slightest difficulty accepting him in the role .\n",
      "\n",
      "[ 0.37284284  0.62715716] ok arthouse . the power of this script , and the performances that come with it , is that the whole damned thing didn't get our moral hackles up .\n",
      "\n",
      "[ 0.46341062  0.53658938] an ill-conceived jumble that's not scary , not smart and not engaging .\n",
      "\n",
      "[ 0.6409392  0.3590608] interesting and thoroughly unfaithful version of carmen\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def do_LOGIT(x,y,xt,yt):\n",
    "    start_t = time.time()\n",
    "    Cs = [0.01,0.1,1]#,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"Usando C= %f\"%C\n",
    "        model = LogisticRegression(penalty='l2',C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"LOGISTIC\")\n",
    "    return model\n",
    "\n",
    "logistic = []\n",
    "print \"Regresión Logística regularizada\"\n",
    "print \"Lematizador con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation')\n",
    "model = do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict_proba(features_test)\n",
    "\n",
    "print \"Lematizador sin Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation',sw=False)\n",
    "model=do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming con StopWords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming')\n",
    "model=do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming sin StopWords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming',sw=False)\n",
    "model=do_LOGIT(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Conjunto aleatorio de prueba\"\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se decidió eliminar los C mayores a 0.1 por tener accurracy 1 en el training set y un menor valor en el test. Por lo tanto se ajustaron mucho a los datos. Se decidió agregar C=1 para tener un valor de una magnitud mayor.\n",
    "\n",
    "Para regresión logística Stemming con el logra mejor accuracy en el training set y test set, a diferencia de los métodos anteriores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(i) SVM **\n",
    "\n",
    "El siguiente paso es construir una función para SVM o Maquinas de vectores de soporte. Al igual de regresión logistica se utiliza un penalty con el parámetro C. Si se repite el mismo comportamiento anterior, para valores de C grande se producirá un ajuste a los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maquina de vectores de soporte\n",
      "Lematizador con Stopwords\n"
     ]
    }
   ],
   "source": [
    "def do_SVM(x,y,xt,yt):\n",
    "    Cs = [0.01,0.1,10,100,1000]\n",
    "    for C in Cs:\n",
    "        print \"El valor de C que se esta probando: %f\"%C\n",
    "        model = LinearSVC(C=C)\n",
    "        model = model.fit(x, y)\n",
    "        score_the_model(model,x,y,xt,yt,\"SVM\")\n",
    "    return model\n",
    "\n",
    "svm = []\n",
    "print \"Maquina de vectores de soporte\"\n",
    "print \"Lematizador con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation')        \n",
    "model = do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "test_pred = model.predict(features_test)\n",
    "\n",
    "print \"Lematizador sin Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('lemmatisation',sw=False)\n",
    "model=do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming con Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming')\n",
    "model=do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Stemming sin Stopwords\"\n",
    "features_train, features_test, labels_train, labels_test, vocab, dist_train = vector_rep('stemming',sw=False)\n",
    "model=do_SVM(features_train,labels_train,features_test,labels_test)\n",
    "\n",
    "print \"Conjunto aleatorio de prueba\"\n",
    "spl = random.sample(xrange(len(test_pred)), 5)\n",
    "for text, sentiment in zip(test_df.Text[spl], test_pred[spl]):\n",
    "    print sentiment, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se predijo antes, se repitió el comportamiento para penalizaciones bajas. El modelo tendió a ajustarse a los datos de entrenamiento. Observando las accuracy para C igual a 0.01 y 0.1 se obtiene una alta accuracy para el entrenamiento y test, siendo mejor para un valor de C=0.1. El mejor modelo es obtenido utilizando stemming por tener un mayor poder predictivo con el data de prueba. Aún así, los demás métodos también alcanzan una accuracy cercana."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(j) Comparación de modelos **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "accuracy_train = []\n",
    "accuracy_test = []\n",
    "precision = []\n",
    "recall = []\n",
    "fscore = []\n",
    "print BNB\n",
    "'''\n",
    "#accuracy_train.extend((BNB[0],multinomial[0],logistic[0],svm[0]))\n",
    "accuracy_test.extend((BNB[1],multinomial[1],logistic[1],svm[1]))\n",
    "precision.extend((BNB[2],multinomial[2],logistic[2],svm[2]))\n",
    "recall.extend((BNB[3],multinomial[3],logistic[3],svm[3]))\n",
    "fscore.extend((BNB[4],multinomial[4],logistic[4],svm[4]))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "index = np.arange(4)\n",
    "bar_width = 0.15\n",
    "opacity = 0.9\n",
    " \n",
    "rects1 = plt.bar(index, accuracy_train, bar_width,alpha=opacity,color='b',label='Accuracy Train')\n",
    "rects2 = plt.bar(index + bar_width, accuracy_test, bar_width,alpha=opacity,color='g',label='Accuracy Test')\n",
    "rects3 = plt.bar(index + 2*bar_width, precision, bar_width,alpha=opacity,color='r',label='Precision')\n",
    "rects4 = plt.bar(index + 3*bar_width, recall, bar_width,alpha=opacity,color='c',label='Recall')\n",
    "rects5 = plt.bar(index + 4*bar_width, fscore, bar_width,alpha=opacity,color='m',label='F1-Score')\n",
    "plt.xlabel('Metodos de Clasificacion')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Metricas')\n",
    "plt.xticks(index + bar_width, ('BernoulliNB', 'MultinomialNB', 'LogisticRegression', 'LinearSVC'))\n",
    "plt.legend()\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
